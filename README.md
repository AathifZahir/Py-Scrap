# ğŸ“š Web Scraping Project with Scrapy and MongoDB

![Build Status](https://img.shields.io/github/actions/workflow/status/yourusername/books_scraper/ci.yml?label=Build) 
![License](https://img.shields.io/github/license/yourusername/books_scraper) 
![Python Version](https://img.shields.io/badge/python-3.9%2B-blue) 
![Scrapy](https://img.shields.io/badge/scrapy-framework-orange) 
![MongoDB](https://img.shields.io/badge/mongodb-database-brightgreen)

---

## ğŸ“‹ Table of Contents

- [ğŸ“– About the Project](#about-the-project)
- [âš™ï¸ Getting Started](#getting-started)
  - [ğŸ“¦ Prerequisites](#prerequisites)
  - [ğŸ”§ Setup Instructions](#setup-instructions)
- [ğŸ’¡ Running the Scraper](#running-the-scraper)
- [ğŸ“‚ Project Structure](#project-structure)
- [âœ¨ Customization](#customization)
- [ğŸ“œ License](#license)
- [ğŸ“š References](#references)

---

## ğŸ“– About the Project

This project demonstrates how to build a web scraper using **Scrapy**, a powerful Python framework for web scraping, and store the extracted data in **MongoDB**, a flexible NoSQL database. 

### Objective

The scraper is designed to extract book information from [atticbooks.co.ke](https://atticbooks.co.ke/) in the **non-fiction books category**. It:
- Extracts relevant data like book titles and prices.
- Handles pagination to scrape data across multiple pages.
- Stores the extracted data in a MongoDB database for further analysis or processing.

---

## âš™ï¸ Getting Started

Follow these steps to get the project running on your local machine. ğŸš€

### ğŸ“¦ Prerequisites

Before running the scraper, ensure you have the following installed:
- ğŸ Python 3.9 or higher
- ğŸ•·ï¸ Scrapy
- ğŸ’¾ MongoDB
- ğŸ”— pymongo

---

### ğŸ”§ Setup Instructions

1. **Clone the Repository**:

   ```bash
   git clone https://github.com/yourusername/books_scraper.git
   cd books_scraper
   ```

2. **Set Up a Virtual Environment**:

   ```bash
   python -m venv venv
   ```

3. **Activate the Virtual Environment**:

   - On Windows:

     ```bash
     venv\Scripts\activate
     ```

   - On Unix or macOS:

     ```bash
     source venv/bin/activate
     ```

4. **Install the Required Packages**:

   ```bash
   pip install scrapy pymongo
   ```

5. **Configure MongoDB**:

   Ensure MongoDB is installed and running on your local machine. The default connection settings in the project are:
   - Host: `localhost`
   - Port: `27017`
   - Database: `books_db`
   - Collection: `books`

   If your MongoDB configuration differs, update the settings in `settings.py` accordingly.

---

## ğŸ’¡ Running the Scraper

To execute the scraper, use the following command:

```bash
scrapy crawl book
```

The scraper will:
- Start at the specified URL.
- Navigate through the non-fiction books category.
- Extract data and store it in the MongoDB database.

---

## ğŸ“‚ Project Structure

The project follows Scrapy's standard structure:

```
books_scraper/
â”œâ”€â”€ books/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ items.py
â”‚   â”œâ”€â”€ middlewares.py
â”‚   â”œâ”€â”€ pipelines.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ spiders/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ book_spider.py
â”œâ”€â”€ scrapy.cfg
â””â”€â”€ README.md
```

- `items.py`: Defines the data structure for the scraped items.
- `pipelines.py`: Contains the pipeline for processing and storing items in MongoDB.
- `settings.py`: Configuration settings for the Scrapy project, including MongoDB connection details.
- `spiders/book_spider.py`: The main spider responsible for scraping the website.

---

## âœ¨ Customization

To adapt the scraper for different categories or websites:

1. **Update the `start_urls`**:

   Modify the `start_urls` list in `book_spider.py` to point to the desired category or website.

2. **Adjust the Parsing Logic**:

   Ensure the CSS selectors in the `parse` method of `book_spider.py` accurately target the desired data fields on the new website.

3. **Handle Pagination**:

   If the target website uses a different pagination structure, update the pagination handling logic in the `parse` method accordingly.

---

## ğŸ“œ License

This project is licensed under the MIT License. See the `LICENSE` file for details. ğŸ“„

---

## ğŸ“š References

For more detailed information on the tools and techniques used in this project, refer to the following resources:
- ğŸ“– [Scrapy Documentation](https://docs.scrapy.org/en/latest/)
- ğŸ—ƒï¸ [Scrapy MongoDB Pipeline](https://github.com/julien-duponchelle/scrapy-mongodb)
- ğŸ“° [Web Scraping With Scrapy and MongoDB](https://realpython.com/web-scraping-with-scrapy-and-mongodb/)

---

## â­ Support

If you like this project, please give it a â­ by clicking the star button at the top of the repository! It helps others discover the project and motivates me to improve it further. â¤ï¸
